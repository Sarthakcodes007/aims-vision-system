{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6019472,"sourceType":"datasetVersion","datasetId":3445072},{"sourceId":7778843,"sourceType":"datasetVersion","datasetId":4551877}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q transformers accelerate datasets pycocotools pillow tqdm\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T07:04:29.485510Z","iopub.execute_input":"2025-08-15T07:04:29.485682Z","iopub.status.idle":"2025-08-15T07:05:50.737879Z","shell.execute_reply.started":"2025-08-15T07:04:29.485666Z","shell.execute_reply":"2025-08-15T07:05:50.737107Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os, sys, random, json, glob\nfrom pprint import pprint\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\nimport matplotlib.pyplot as plt\n\n# Disable tokenizer parallelism warnings\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nprint(\"Python:\", sys.version.splitlines()[0])\nprint(\"PyTorch:\", torch.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"GPU:\", torch.cuda.get_device_name(0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T07:05:50.738915Z","iopub.execute_input":"2025-08-15T07:05:50.739248Z","iopub.status.idle":"2025-08-15T07:06:24.291740Z","shell.execute_reply.started":"2025-08-15T07:05:50.739213Z","shell.execute_reply":"2025-08-15T07:06:24.291098Z"}},"outputs":[{"name":"stderr","text":"2025-08-15 07:06:04.814932: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755241565.167081      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755241565.267713      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Python: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\nPyTorch: 2.6.0+cu124\nCUDA available: True\nGPU: Tesla T4\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Change these to match your Kaggle paths\nREFCOCO_ANN = \"/kaggle/input/refcoco/anns/refcoco/train.json\"\nCOCO_IMG_DIR = \"/kaggle/input/coco-image-caption/train2014/train2014\"\n\nprint(\"Loading annotations from:\", REFCOCO_ANN)\nwith open(REFCOCO_ANN, 'r') as f:\n    ann = json.load(f)\n\nprint(\"Annotation entries:\", len(ann))\nprint(\"Example entry:\")\npprint(ann[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T07:11:18.892905Z","iopub.execute_input":"2025-08-15T07:11:18.893683Z","iopub.status.idle":"2025-08-15T07:11:19.229816Z","shell.execute_reply.started":"2025-08-15T07:11:18.893657Z","shell.execute_reply":"2025-08-15T07:11:19.229084Z"}},"outputs":[{"name":"stdout","text":"Loading annotations from: /kaggle/input/refcoco/anns/refcoco/train.json\nAnnotation entries: 42404\nExample entry:\n{'bbox': [103, 299, 237, 476],\n 'cat': 0,\n 'img_name': 'COCO_train2014_000000581857.jpg',\n 'segment_id': 0,\n 'sentences': [{'idx': 0, 'sent': 'the lady with the blue shirt', 'sent_id': 0},\n               {'idx': 1, 'sent': 'lady with back to us', 'sent_id': 1},\n               {'idx': 2, 'sent': 'blue shirt', 'sent_id': 2}],\n 'sentences_num': 3}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def build_examples(annotation_list, images_root):\n    examples = []\n    for ann in annotation_list:\n        img_path = os.path.join(images_root, ann['img_name'])\n        bbox = ann['bbox']  # [x1, y1, x2, y2]\n        for s in ann['sentences']:\n            examples.append({\n                \"image_path\": img_path,\n                \"text\": s['sent'],\n                \"bbox\": bbox\n            })\n    return examples\n\nexamples = build_examples(ann, COCO_IMG_DIR)\nprint(f\"Total examples: {len(examples)}\")\nprint(\"First example:\", examples[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T07:11:27.951763Z","iopub.execute_input":"2025-08-15T07:11:27.952074Z","iopub.status.idle":"2025-08-15T07:11:28.442321Z","shell.execute_reply.started":"2025-08-15T07:11:27.952034Z","shell.execute_reply":"2025-08-15T07:11:28.441546Z"}},"outputs":[{"name":"stdout","text":"Total examples: 120624\nFirst example: {'image_path': '/kaggle/input/coco-image-caption/train2014/train2014/COCO_train2014_000000581857.jpg', 'text': 'the lady with the blue shirt', 'bbox': [103, 299, 237, 476]}\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def build_examples(annotation_list, images_root):\n    examples = []\n    for ann in annotation_list:\n        img_path = os.path.join(images_root, ann['img_name'])\n        bbox = ann['bbox']  # [x1, y1, x2, y2]\n        for s in ann['sentences']:\n            examples.append({\n                \"image_path\": img_path,\n                \"text\": s['sent'],\n                \"bbox\": bbox\n            })\n    return examples\n\nexamples = build_examples(ann, COCO_IMG_DIR)\nprint(f\"Total examples: {len(examples)}\")\nprint(\"First example:\", examples[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T07:11:37.630335Z","iopub.execute_input":"2025-08-15T07:11:37.630603Z","iopub.status.idle":"2025-08-15T07:11:37.739321Z","shell.execute_reply.started":"2025-08-15T07:11:37.630582Z","shell.execute_reply":"2025-08-15T07:11:37.738625Z"}},"outputs":[{"name":"stdout","text":"Total examples: 120624\nFirst example: {'image_path': '/kaggle/input/coco-image-caption/train2014/train2014/COCO_train2014_000000581857.jpg', 'text': 'the lady with the blue shirt', 'bbox': [103, 299, 237, 476]}\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"random.shuffle(examples)\ntrain_examples = examples[:int(0.8 * len(examples))]\nval_examples   = examples[int(0.8 * len(examples)):]\n\nprint(f\"Train: {len(train_examples)}, Val: {len(val_examples)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T07:11:43.786065Z","iopub.execute_input":"2025-08-15T07:11:43.786342Z","iopub.status.idle":"2025-08-15T07:11:43.836880Z","shell.execute_reply.started":"2025-08-15T07:11:43.786318Z","shell.execute_reply":"2025-08-15T07:11:43.836088Z"}},"outputs":[{"name":"stdout","text":"Train: 96499, Val: 24125\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def xyxy_to_cxcywh_norm(box_xyxy, img_w, img_h):\n    x1, y1, x2, y2 = box_xyxy\n    w = x2 - x1\n    h = y2 - y1\n    cx = x1 + w / 2.0\n    cy = y1 + h / 2.0\n    return torch.tensor([cx/img_w, cy/img_h, w/img_w, h/img_h], dtype=torch.float32)\n\ndef cxcywh_to_xyxy(box):\n    cx, cy, w, h = box.unbind(-1)\n    x1 = cx - 0.5 * w\n    y1 = cy - 0.5 * h\n    x2 = cx + 0.5 * w\n    y2 = cy + 0.5 * h\n    return torch.stack([x1, y1, x2, y2], dim=-1)\n\ndef iou_cxcywh(pred, target, eps=1e-6):\n    p_xy = cxcywh_to_xyxy(pred)\n    t_xy = cxcywh_to_xyxy(target)\n    x1 = torch.max(p_xy[...,0], t_xy[...,0])\n    y1 = torch.max(p_xy[...,1], t_xy[...,1])\n    x2 = torch.min(p_xy[...,2], t_xy[...,2])\n    y2 = torch.min(p_xy[...,3], t_xy[...,3])\n    inter = (x2 - x1).clamp(min=0) * (y2 - y1).clamp(min=0)\n    area_p = (p_xy[...,2] - p_xy[...,0]).clamp(min=0) * (p_xy[...,3] - p_xy[...,1]).clamp(min=0)\n    area_t = (t_xy[...,2] - t_xy[...,0]).clamp(min=0) * (t_xy[...,3] - t_xy[...,1]).clamp(min=0)\n    return inter / (area_p + area_t - inter + eps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T07:11:51.992585Z","iopub.execute_input":"2025-08-15T07:11:51.993312Z","iopub.status.idle":"2025-08-15T07:11:52.004734Z","shell.execute_reply.started":"2025-08-15T07:11:51.993284Z","shell.execute_reply":"2025-08-15T07:11:52.004004Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class RefCOCODataset(Dataset):\n    def __init__(self, examples, processor):\n        self.examples = examples\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, idx):\n        ex = self.examples[idx]\n        image = Image.open(ex[\"image_path\"]).convert(\"RGB\")\n        text = [ex[\"text\"]]\n        img_w, img_h = image.size\n        gt_box = xyxy_to_cxcywh_norm(torch.tensor(ex[\"bbox\"], dtype=torch.float32), img_w, img_h)\n\n        enc = self.processor(\n            images=image,\n            text=text,\n            padding=\"max_length\",\n            max_length=16,\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n\n        enc = {k: v.squeeze(0) if isinstance(v, torch.Tensor) else v for k,v in enc.items()}\n        enc[\"gt_box\"] = gt_box\n        return enc\n\ndef collate_fn(batch):\n    return {\n        \"pixel_values\": torch.stack([b[\"pixel_values\"] for b in batch]),\n        \"input_ids\": torch.stack([b[\"input_ids\"] for b in batch]),\n        \"attention_mask\": torch.stack([b[\"attention_mask\"] for b in batch]),\n        \"gt_box\": torch.stack([b[\"gt_box\"] for b in batch])\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T07:12:05.028174Z","iopub.execute_input":"2025-08-15T07:12:05.028886Z","iopub.status.idle":"2025-08-15T07:12:05.038026Z","shell.execute_reply.started":"2025-08-15T07:12:05.028853Z","shell.execute_reply":"2025-08-15T07:12:05.037293Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"MODEL_ID = \"google/owlvit-base-patch32\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 2\nACCUM_STEPS = 4\nEPOCHS = 20  # train fully\nLR = 5e-6\nUSE_AMP = True\nsave_dir = \"owlvit-refcoco-finetuned\"\nos.makedirs(save_dir, exist_ok=True)\n\nprocessor = OwlViTProcessor.from_pretrained(MODEL_ID)\nmodel = OwlViTForObjectDetection.from_pretrained(MODEL_ID).to(DEVICE)\n\ntrain_loader = DataLoader(RefCOCODataset(train_examples, processor), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\nval_loader   = DataLoader(RefCOCODataset(val_examples, processor), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR)\nscaler = torch.amp.GradScaler(enabled=USE_AMP)\n\nresume_checkpoint = os.path.join(save_dir, \"checkpoint.pth\")\nstart_epoch = 0\nbest_val_loss = float(\"inf\")\n\nif os.path.exists(resume_checkpoint):\n    print(f\"Resuming from {resume_checkpoint}\")\n    checkpoint = torch.load(resume_checkpoint, map_location=DEVICE)\n    model.load_state_dict(checkpoint[\"model\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n    scaler.load_state_dict(checkpoint[\"scaler\"])\n    start_epoch = checkpoint[\"epoch\"] + 1\n    best_val_loss = checkpoint[\"best_val_loss\"]\n    print(f\"Resumed at epoch {start_epoch}, best_val_loss={best_val_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T07:12:17.053832Z","iopub.execute_input":"2025-08-15T07:12:17.054676Z","iopub.status.idle":"2025-08-15T07:12:22.410297Z","shell.execute_reply.started":"2025-08-15T07:12:17.054641Z","shell.execute_reply":"2025-08-15T07:12:22.409693Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/392 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2cb276745954cbc9facf1684d50d394"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/775 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"162fee24af4841b3ad212801da34c532"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94a1c966b9e7414eb1ad6ab92fd09c70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"547a812694504455a6dd6a5e02fe6adf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/460 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f97beb94f8a54a049c5ef6351e42d9d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"789216c100a24b91887de69a1bca3b3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/613M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf93abc5ca444624bbd887f253cc249b"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\nfor epoch in range(start_epoch, EPOCHS):\n    model.train()\n    running_loss = 0.0\n    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n    for step, batch in pbar:\n        batch = {k: v.to(DEVICE) if torch.is_tensor(v) else v for k,v in batch.items()}\n        with torch.amp.autocast(\"cuda\", enabled=USE_AMP):\n            outputs = model(\n                input_ids=batch[\"input_ids\"],\n                pixel_values=batch[\"pixel_values\"],\n                attention_mask=batch[\"attention_mask\"]\n            )\n\n            pred_boxes = outputs.pred_boxes\n            gt_box = batch[\"gt_box\"]\n            B, Q, _ = pred_boxes.shape\n            gt_box_exp = gt_box.unsqueeze(1).expand(-1, Q, -1)\n            ious = iou_cxcywh(pred_boxes, gt_box_exp)\n            best_idx = torch.argmax(ious, dim=1)\n            chosen_preds = pred_boxes[torch.arange(B), best_idx]\n            chosen_ious = ious[torch.arange(B), best_idx]\n            l1 = torch.nn.functional.l1_loss(chosen_preds, gt_box, reduction=\"none\").mean(dim=1)\n            loss = (l1 + (1.0 - chosen_ious)).mean()\n\n        scaler.scale(loss / ACCUM_STEPS).backward()\n\n        if (step + 1) % ACCUM_STEPS == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n\n        running_loss += loss.item()\n        if (step + 1) % 50 == 0:\n            pbar.set_description(f\"epoch {epoch+1} loss {running_loss/(step+1):.4f}\")\n\n    # Validation\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(DEVICE) if torch.is_tensor(v) else v for k,v in batch.items()}\n            with torch.amp.autocast(\"cuda\", enabled=USE_AMP):\n                outputs = model(\n                    input_ids=batch[\"input_ids\"],\n                    pixel_values=batch[\"pixel_values\"],\n                    attention_mask=batch[\"attention_mask\"]\n                )\n                pred_boxes = outputs.pred_boxes\n                gt_box = batch[\"gt_box\"]\n                B, Q, _ = pred_boxes.shape\n                gt_box_exp = gt_box.unsqueeze(1).expand(-1, Q, -1)\n                ious = iou_cxcywh(pred_boxes, gt_box_exp)\n                best_idx = torch.argmax(ious, dim=1)\n                chosen_preds = pred_boxes[torch.arange(B), best_idx]\n                chosen_ious = ious[torch.arange(B), best_idx]\n                l1 = torch.nn.functional.l1_loss(chosen_preds, gt_box, reduction=\"none\").mean(dim=1)\n                val_loss += (l1 + (1.0 - chosen_ious)).mean().item()\n    val_loss /= len(val_loader)\n\n    print(f\"Epoch {epoch+1} train_loss {running_loss/len(train_loader):.4f} val_loss {val_loss:.4f}\")\n\n    # Save best model\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        model.save_pretrained(save_dir)\n        processor.save_pretrained(save_dir)\n        print(f\"✅ Saved best model at epoch {epoch+1}\")\n\n    # Save checkpoint\n    torch.save({\n        \"epoch\": epoch,\n        \"model\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n        \"scaler\": scaler.state_dict(),\n        \"best_val_loss\": best_val_loss\n    }, resume_checkpoint)\n    print(f\"💾 Checkpoint saved at epoch {epoch+1}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T07:12:34.322127Z","iopub.execute_input":"2025-08-15T07:12:34.322415Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/48250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04351e64d3b345e5bc04c3c59cb8f6f1"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}